{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hw2_Part4_distribute.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaA1H2AHyVWQ"
      },
      "source": [
        "# ML-2: Trees, Model Interrogation and Bayesian Workflow\n",
        "# Homework 2: Rossman Kaggle: Forecasting Sales\n",
        "# Part 4 : Modelling with embeddings!\n",
        "**ML-2 Cohort 1** <br>\n",
        "**Instructor: Dr. Rahul Dave**<br>\n",
        "**Max Score: 100** <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jBFtovs3nh1"
      },
      "source": [
        "#importing libraries\n",
        "import numpy as np\n",
        "import scipy.stats\n",
        "import scipy.special\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.mlab as mlab\n",
        "from matplotlib import cm\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import make_pipeline, make_union, Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import ParameterGrid, train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.models import Model as KerasModel\n",
        "from keras.layers import Input, Dense, Activation, Reshape\n",
        "from keras.layers import Concatenate\n",
        "from keras.layers.embeddings import Embedding\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn import linear_model\n",
        "import pickle\n",
        "import csv\n",
        "from datetime import datetime\n",
        "from sklearn import preprocessing\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "%matplotlib inline"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlux5Gn-n-Gz"
      },
      "source": [
        "We will repeat the first initial steps again from Part 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5Hi8ehR7WFX"
      },
      "source": [
        "Lets import the feature_train_data.pickle file and set X,y values from the pickle file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arl_aj6X7VQk"
      },
      "source": [
        "#your code here\n",
        "f = open('feature_train_data.pickle', 'rb') \n",
        "(X, y) = pickle.load(f)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwMxtFTUDRFN"
      },
      "source": [
        "# we will split the train_ratio and 90% and 10% and set the train_size\n",
        "train_ratio = 0.9\n",
        "num_records = len(X)\n",
        "train_size = int(train_ratio * num_records)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpP1Emg2Dni0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "246a9a4e-9c2d-4fb1-d6a2-065cb57d5f64"
      },
      "source": [
        "#lets look at our data\n",
        "X[1], y[1]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([   0, 1058,    1,    0,    0,    0,    0,    1]), 4491)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2fhYw49Dz3w"
      },
      "source": [
        "The next set of inputs is following: Write the code for this yourself\n",
        "\n",
        "1. Do you want to one hot encode the data?\n",
        "2. Do you want to provide embeddings as input - this will be set to True for models with entity embeddings\n",
        "3. Do you want to save the emmbeddings? - again set to true if you want to entity embeddings\n",
        "4. if 3 is set to true, we want to save them to a embeddings.pickle\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tqtv6VWWDuZu"
      },
      "source": [
        "#your code here\n",
        "one_hot_as_input = False #one_hot is set to False\n",
        "embeddings_as_input = True #embeddings later on needs to set to true for Part 3\n",
        "save_embeddings = True\n",
        "saved_embeddings_fname = \"embeddings.pickle\"  # set save_embeddings to True to create this file"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ayLSo_1E0yF"
      },
      "source": [
        "## Now lets work with Models with Entity embedding!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTYK-isysXiu"
      },
      "source": [
        "Now that you have saved embeddings - push this into the other models as an input with X. \n",
        "\n",
        "How will we do this? \n",
        "\n",
        "We need to update our X values. \n",
        "\n",
        "1. We will define a function called embed_features, which will combine the embedding with X. \n",
        "2. Call this function and update it with the inital X values taken from the pickle file - features_train_data\n",
        "3. Then split you data, X_emb - into Xtrain and X_Val, y_train and y_val remain the same\n",
        "4. Sample the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbymoNn65ETi"
      },
      "source": [
        "#call our saved embeddings from part 3\n",
        "\n",
        "saved_embeddings_fname = '/content/embeddings.pickle'"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUVh51hgqCyp"
      },
      "source": [
        "#combining embedding\n",
        "def embed_features(X, saved_embeddings_fname):\n",
        "    f_embeddings = open(saved_embeddings_fname, \"rb\")\n",
        "    embeddings = pickle.load(f_embeddings)\n",
        "\n",
        "    index_embedding_mapping = {1: 0, 2: 1, 4: 2, 5: 3, 6: 4, 7: 5}\n",
        "    X_embedded = []\n",
        "\n",
        "    (num_records, num_features) = X.shape\n",
        "    for record in X:\n",
        "        embedded_features = []\n",
        "        for i, feat in enumerate(record):\n",
        "            feat = int(feat)\n",
        "            if i not in index_embedding_mapping.keys():\n",
        "                embedded_features += [feat]\n",
        "            else:\n",
        "                embedding_index = index_embedding_mapping[i]\n",
        "                embedded_features += embeddings[embedding_index][feat].tolist()\n",
        "\n",
        "        X_embedded.append(embedded_features)\n",
        "\n",
        "    return np.array(X_embedded)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hf8U7znjHZrW"
      },
      "source": [
        "**Explain what is happening the function above**\n",
        "\n",
        "your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALCfACl0rzeK"
      },
      "source": [
        "### Embedding with X - input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wcLbyHps8_W"
      },
      "source": [
        "#check if embedding is needed, if yes call embed_features - with X and the embeddings passed to it - call this new X as X_emb\n",
        "if embeddings_as_input:\n",
        "   X_emb = embed_features(X, saved_embeddings_fname)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDzhYvCJHm8d"
      },
      "source": [
        "Split the train and validation based on train size and on the new X_emb values from the previous code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLTrTSGPEaGw"
      },
      "source": [
        "#update the X_train and X_val\n",
        "#your code here \n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_emb, y, train_size= train_size, random_state=42)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StnmulRkEnRt"
      },
      "source": [
        "def sample(X, y, n):\n",
        "    '''random samples'''\n",
        "    num_row = X.shape[0]\n",
        "    indices = np.random.randint(num_row, size=n)\n",
        "    return X[indices, :], y[indices]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ednNv14aEpjS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac673450-991c-4441-d3c1-b6a112785730"
      },
      "source": [
        "X_train, y_train = sample(X_train, y_train, 200000)  # Simulate data sparsity\n",
        "print(\"Number of samples used for training: \" + str(y_train.shape[0]))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples used for training: 200000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1I6AT5MuRTH"
      },
      "source": [
        "## Add the embeddings into the Models and check their MAPE!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmJYNA0PH5AU"
      },
      "source": [
        "All the models defined here will have the same parameters as the ones defined in Part 2!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTRegclpugs3"
      },
      "source": [
        "#define MAPE Function here\n",
        "#your code here\n",
        "\n",
        "def MAPE(Y_actual,Y_Predicted):\n",
        "  return np.mean(np.abs((Y_actual - Y_Predicted) / Y_actual)) * 100"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4Sjp7NQu1ow"
      },
      "source": [
        "### Model 1: Random Forests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGX-_sRYkxPv"
      },
      "source": [
        "#your code here\n",
        "\n",
        "rf_reg = RandomForestRegressor(n_estimators = 200, max_depth = 35, min_samples_split = 2, min_samples_leaf = 1)\n",
        "rf_reg.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9Tfa-4vJg8x"
      },
      "source": [
        "rf_train_pred = rf_reg.predict(X_train)\n",
        "rf_val_pred = rf_reg.predict(X_val)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxnjC3ZcGtKk",
        "outputId": "6164540c-453c-4cb4-ca68-7a16d1c23489"
      },
      "source": [
        "print(\"The mean absolute percentage error on training data is \", np.round(MAPE(y_train, rf_train_pred), 3))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The mean absolute percentage error on training data is  2.847\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2f9uHMoGv-K",
        "outputId": "b7064114-26b8-4af9-808e-948fe62404a5"
      },
      "source": [
        "print(\"The mean absolute percentage error on validation data is \", np.round(MAPE(y_val, rf_val_pred), 3))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The mean absolute percentage error on validation data is  9.076\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYvVmYMpu_Vi"
      },
      "source": [
        "### Model 2: Boosting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1--s_4pGOuB"
      },
      "source": [
        "#your code here\n",
        "DM_train = xgb.DMatrix(data = X_train, \n",
        "                       label = y_train)\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNdYL42lG2qc"
      },
      "source": [
        "param  = {'nthread' : -1, \n",
        "           'max_depth': 7, \n",
        "           'eta': 0.02,\n",
        "           'silent': 1,\n",
        "           'objective': 'reg:linear',\n",
        "           'colsample_bytree': 0.7,\n",
        "           'subsample': 0.7}\n",
        "\n",
        "num_round = 3000"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoJdH3F9G5E5"
      },
      "source": [
        "bst = xgb.train(param, DM_train, num_round)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_6eOI1IG6kF"
      },
      "source": [
        "feature_Xtr = xgb.DMatrix(X_train)\n",
        "feature_Xval = xgb.DMatrix(X_val)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9C361Yj-G8BH"
      },
      "source": [
        "xgb_y_pred_train = bst.predict(feature_Xtr)\n",
        "xgb_y_pred_val = bst.predict(feature_Xval)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyo7wsykG8ER"
      },
      "source": [
        "xgb_mape_train= MAPE(y_train, xgb_y_pred_train)\n",
        "xgb_mape_val= MAPE(y_val, xgb_y_pred_val)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFesvp60G-3L",
        "outputId": "1fe7b446-51a0-4b81-9cea-f9adb80c102b"
      },
      "source": [
        "print(\"The mean absolute percentage error on training data is \", np.round(xgb_mape_train, 3))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The mean absolute percentage error on training data is  5.99\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXqnI8L0G-7Y",
        "outputId": "50656645-0396-4976-eada-e56cc08691ce"
      },
      "source": [
        "print(\"The mean absolute percentage error on validation data is \", np.round(xgb_mape_val, 3))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The mean absolute percentage error on validation data is  7.183\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhM6IVJJv0rU"
      },
      "source": [
        "## Final Commments!\n",
        "\n",
        "Apart from how long this homework was, lets make some other final comments\n",
        "\n",
        "**Question 1: Did you models with Entity Embeddings perfom better?**\n",
        "\n",
        "No, it did not perform better. \n",
        "\n",
        "\n",
        "**Question 2: Now that you have completed this homework, lets answer the main purpose of the homework - How do you think entity embeddings improved the MAPE score. To show this do a similar table like the one did in Paper**\n",
        "\n",
        "*your answer here*\n",
        "\n",
        "\n",
        "**Question 3: Add a table here to show the similar to the one done in paper - to show the different MAPE values with and without embeddings**\n",
        "\n",
        "*your answer here*"
      ]
    }
  ]
}